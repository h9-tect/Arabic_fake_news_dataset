{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "cd ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnc7rwoF61Nf",
        "outputId": "cebbab19-c459-49a8-9df0-5182ba67983c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "base_url = \"https://matsda2sh.com/list/26/%D8%A7%D8%AC%D8%AA%D9%85%D8%A7%D8%B9%D9%8A?page={}\"\n",
        "\n",
        "\n",
        "# Function to extract links from a given page\n",
        "def extract_links_from_page(url):\n",
        "    response = requests.get(url)\n",
        "    html_content = response.text\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "    media_elements = soup.find_all(\"div\", class_=\"media\")\n",
        "    links = []\n",
        "    for media_element in media_elements:\n",
        "        anchor_tags = media_element.find_all(\"a\", href=True)\n",
        "        for anchor_tag in anchor_tags:\n",
        "            link = anchor_tag[\"href\"]\n",
        "            links.append(link)\n",
        "    return links\n",
        "\n",
        "# Function to scrape specified classes from a given link's page\n",
        "def scrape_classes_from_link(url):\n",
        "    response = requests.get(url)\n",
        "    html_content = response.text\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "    # Scrape class=\"wekek blog-section references-section\" and label as \"fake\"\n",
        "    fake_elements = soup.find_all(\"div\", class_=\"wekek blog-section references-section\")\n",
        "    fakes = [fake.text.strip() for fake in fake_elements]\n",
        "\n",
        "    # Scrape class=\"wp-editor conclusion-ddd\" and label as \"true\"\n",
        "    true_elements = soup.find_all(\"div\", class_=\"wp-editor conclusion-ddd\")\n",
        "    trues = [true.text.strip() for true in true_elements]\n",
        "\n",
        "    return fakes, trues\n",
        "\n",
        "# Get the number of pages (you can get this information programmatically or manually)\n",
        "num_pages = 47\n",
        "\n",
        "# Scrape data from all pages and store in a list\n",
        "data_list = []\n",
        "for page_number in range(1, num_pages + 1):\n",
        "    url = base_url.format(page_number)\n",
        "    links_on_page = extract_links_from_page(url)\n",
        "    for link in links_on_page:\n",
        "        fakes, trues = scrape_classes_from_link(link)\n",
        "        data = {\n",
        "            \"link\": link,\n",
        "            \"fakes\": fakes,\n",
        "            \"trues\": trues\n",
        "        }\n",
        "        data_list.append(data)\n",
        "\n",
        "# Write the data into a JSON file\n",
        "with open(\"fake_news_dataset_socio.json\", \"w\") as json_file:\n",
        "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Dataset created and saved as 'fake_news_dataset.json'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14apeO8Xv3Ki",
        "outputId": "2fc43d38-a47e-4287-c947-3a3651e41218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created and saved as 'fake_news_dataset.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Load the individual JSON files\n",
        "with open('fake_news_dataset_socio.json') as f:\n",
        "    socio_data = json.load(f)\n",
        "\n",
        "with open('fake_news_dataset.json') as f:\n",
        "    general_data = json.load(f)\n",
        "\n",
        "with open('fake_news_dataset_eco.json') as f:\n",
        "    eco_data = json.load(f)\n",
        "\n",
        "with open('fake_news_dataset_politic.json') as f:\n",
        "    politic_data = json.load(f)\n",
        "\n",
        "with open('fake_news_dataset_sci.json') as f:\n",
        "    sci_data = json.load(f)\n",
        "\n",
        "with open('fake_news_dataset_sport.json') as f:\n",
        "    sport_data = json.load(f)\n",
        "\n",
        "with open('fake_news_dataset_tech.json') as f:\n",
        "    tech_data = json.load(f)\n",
        "\n",
        "# Combine the datasets\n",
        "combined_data = (\n",
        "    socio_data\n",
        "    + general_data\n",
        "    + eco_data\n",
        "    + politic_data\n",
        "    + sci_data\n",
        "    + sport_data\n",
        "    + tech_data\n",
        ")\n",
        "\n",
        "# Shuffle the combined data\n",
        "random.shuffle(combined_data)\n",
        "\n",
        "# Save the shuffled data to a new JSON file\n",
        "with open('arabic_fake_news_dataset.json', 'w') as f:\n",
        "    json.dump(combined_data, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "Exaac8831Rst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"arabic_fake_news_dataset.json\"\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    # Get the size of the file in bytes\n",
        "    file_size_bytes = os.path.getsize(file_path)\n",
        "\n",
        "    # Convert bytes to megabytes (MB)\n",
        "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "\n",
        "    print(f\"The size of 'arabic_fake_news_dataset.json' is {file_size_mb:.2f} MB.\")\n",
        "else:\n",
        "    print(\"File not found.\")\n"
      ],
      "metadata": {
        "id": "9WUGwIFGHKWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe488cc9-3d29-4e1b-b1c4-cfd061a1ce6b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of 'arabic_fake_news_dataset.json' is 7.65 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pr0l4AFtLIAw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}